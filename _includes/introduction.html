<!-- Introduction -->
<section class="page-section" id="introduction">
  <div class="container">

    <div class="row">
      <div class="col-lg-12 text-center">
        <h2 class="section-heading text-uppercase mb-3 tb-3">Introduction</h2>

        <div class="section-subheading text-justify lead">
          Standing in front of us is a dataset of thousands of beer reviews taken from 
          the BeerAdvocate <a
          href="https://www.beeradvocate.com/">BeerAdvocate</a> website. Each reviewer has given a 1-5 rating for each 
          of the four beer aspects: Aroma, Palate, Appearance and Taste, along with an Overall 
          grade. In addition, they also provided a textual review further explaining their 
          opinion on a specific beer. In a nutshell, our aim is to explore what is it we write 
          in these reviews that influences the ratings significantly. These conclusions can 
          help breweries adjust their production according to people’s preferences and improve
           targeting: Do people often say that a beer is too sweet? It is an indicator to reduce 
           sugar during the brewing process. On the other hand, are taste and appearance rated well?
            The breweries can then focus their commercials on slow-motion close-ups of a person
             drinking from a transparent glass, with their mouth in the forefront.
        <br><br>

             So, we will focus on answering two main questions:
             <h5 class="section-heading mt-3 mb-3 tb-3">~ Which beer aspects are the most important 
              for users? Namely, what are the aspects 
              that correlate positively with their overall rating?</h5>
             <h5 class="section-heading mt-3 mb-3 tb-3">~ What keywords do reviewers use 
              about these aspects that are decisive 
              factors when they give good grades?</h5>

          <br>

          One might ask, why do we even introduce textual analysis in our work? 
            Okay, for our second question, the answer is quite straightforward – we need 
            it to <u><i>actually</i></u> extract important words. But what about the first one? 
            Aren't the grades users give enough to infer if an aspect stands out as 
            influential? Well, there are two problems that arise.
            <br><br>
          
            First, people mostly give good grades. As can be seen from <em>Figure 1</em>, 
           a vast majority of reviews (>90%) have numerical ratings all higher than 3.
            Therefore, if a certain aspect indeed stands out, its difference in rating compared 
            to the other ones will not be so easy to notice.

            <div class="col-md-12">
              <h6 class="subheading text-center">Figure 1</h6>
              <div>
                {% include small_grades.html %}
              </div>
            </div>


          <h5 class="section-heading mt-3 mb-3 tb-3">Motivation</h5>
          <div>
            One might ask, why would we dive into exploring a dataset that is self-explanatory: we have the ratings - we map them to beer quality. 
            After all, don’t people know what beer they like and what they don’t like, so they will rate them accordingly? 
            Well, that’s not always the case. In fact, there is a famous phenomenon called the *Halo Effect*, empirically proven by a psychologist Edward Thorndike at the 
            beginning of the last century. It is the tendency for positive impressions of a person, company, brand, or product in one area to positively 
            influence one’s opinion or feelings in other areas. For example, if we see an attractive, well groomed, and properly attired person, 
            we will probably assume that they are a good person. This is exploited by felons on trials when they perfect their physical appearance 
            in order to appeal to the jury. A marketing example would be the case of Apple, where the popularity of the iPhone generated enthusiasm 
            towards other products of the Apple ecosystem.
            Belowe, we see that the Halo Effect is very frequent and spreads over many areas. Therefore, it is a rational assumption that the effect is also present in our dataset 
            of beer reviews. We aim to see if one of the beer aspects that the consumer particularly liked influenced its opinion towards other aspects i.e., led to him 
            rating all beer aspects in a similar manner. A glimpse of our dataset shows that around 56% of reviews have numerical aspect ratings almost coincide:
            
 <br><br>

            {% include aspects_same.html %}

            This is an indicator that we’re going in the right direction, as it’s statistically very unlikely that each beer aspect follows the same pattern.
            Okay, we’re on a good start, but how do we know which aspect makes a difference and find out if there are some common for most of the graders? 
            Well, we could see what the users *actually* had to say about the beer they rated. In other words, we will look for their opinion in the review text they wrote. 
            That’s where our good old BERT-based sentiment analysis model (or BEERT as we like to call it) steps in. It provides us with the positive/neutral/negative sentiment 
            score from the text, for each of the four aspects. If the users focus on one specific aspect, while ignoring the others or even speaking negatively about them, 
            we will know that one makes a difference.

            {% include small_grades.html %}
            
            
          </div><br>

          <h5 class="section-heading mt-3 mb-3 tb-3">Where does the trail of breadcrumbs begin?</h5>
          <div>
            To answer the questions of interest we will take a look inside the <a class="smaller-font quotebank"
              href="https://dlab.epfl.ch/people/west/pub/Vaucher-Spitz-Catasta-West_WSDM-21.pdf">Quotebank<span
                class="quotebank-dot">.</span></a>, a dataset of 178 million
            quotations. However, when only observing US politicians the set is reduced to around 5.7 million quotes
            which is still an ocean of claims. In order to find the needed answers, we must define what it means for a
            quote to be related to climate change. Superficially the problem might seem negligible, but it is inherently
            difficult and necessary for us to narrow down the data closer to our problem space.</div><br>

          <h5 class="section-heading mt-3 mb-3 tb-3">How do we know if a quote is about climate change?</h5>
          <div>
            To solve this problem we will use the power of data to define the data. We can do this in two ways. Firstly,
            we can let the data speak for itself by using an unsupervised technique. Secondly, we can base our
            definition on an external dataset that has been labeled by a human and rely on their definition of the
            problem. Let us look at both techniques.
            <li>
              <h6 class="inline section-heading mt-3 mb-1 tb-3">FastText (unsupervised)</h6>
              <div class="indent">
                This model is able to train on a given corpus of text and learn sentence representation in a vector
                format. The quotes had to be first cleaned using stop word removal, lower-casing and removing
                punctuation. After training the model we need to select what is known as a "query". This is a sentence
                which would be embedded and compared using cosine similarity to find the most similar quotes. The query
                selected for the purpose of extracting climate change related quotes was "climate change" based on the
                results on the validation set.
              </div>
            </li>
            <li>
              <h6 class="inline section-heading mt-3 mb-1 tb-3">BERT (supervised)</h6>
              <div class="indent">
                As previously stated we must have a labeled dataset for a supervised approach. Luckily, there is such a
                dataset, <a
                  href="https://www.sustainablefinance.uzh.ch/en/research/climate-fever/climatext.html">ClimaText</a>.
                As the paper did not provide a trained model for this task we need to train one. The model used for this
                is the <a href="https://huggingface.co/">Huggingface</a> implementation of distilled BERT for sequence
                classification. After training, the
                model returns the probability of a quote being related to climate change.
              </div>
            </li>

            <h5 class="smaller-font inline section-heading mt-3 tb-3">Comparison</h5>
            {% include tab2.html %}


            <br>
            <h5 class="smaller-font inline section-heading mt-1 mb-1 tb-3">Two precision/recall curves</h5>
            <div class="col-sm-12">
              {% include prc.html %}
            </div>
            <div>
              The general performance of the supervised technique is higher, for this reason, we decide to use this
              model
              to estimate the probability of a quote to be related to climate change.
              <br><br>We continue by narrowing the dataset of 5.7 million quotes to a set of around 47000 by selecting
              only those classified as being about climate change.
            </div>
          </div>

          <br><br>
          <div class="row">
            <div class="col-sm-12 text-center">
              WOW! That was a loooong introduction, so congratulations on getting this far. Now, let's cut to the chase
              and get our hands dirty!
              <br>
              In the words of the former US president: <q><em>What the hell is going on with Global Waming?</em></q>
              <div class="tweet">
                <img src="assets/img/trumpClimateChange.jpg" alt="" srcset="">
              </div>
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>

</section>
<!-- End Introduction -->